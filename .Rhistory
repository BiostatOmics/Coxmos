best_splsdrcox_mixOmics$nzv
best_splsdrcox_mixOmics$survival_model$fit
best_splsdrcox_mixOmics
aux_folder = paste0(folder, "splsdacox_mixOmics_plot/")
dir.create(aux_folder)
cv.splsdacox_mixOmics_res <- HDcox::cv.splsdacox_mixOmics(X = X_train, Y = Y_train,
max.ncomp = max.ncomp, vector = vector,
MIN_NVAR = MIN_NVAR, MAX_NVAR = MAX_NVAR, n.cut_points = n.cut_points,
n_run = n_run, k_folds = k_folds,
alpha = alpha, remove_non_significant = remove_non_significant, times = times,
remove_near_zero_variance = remove_near_zero_variance,
remove_zero_variance = remove_zero_variance,
remove_non_significant_models = remove_non_significant_models,
toKeep.zv = toKeep.zv,
w_AIC = w_AIC, w_c.index = w_c.index, w_AUC = w_AUC,
MIN_AUC_INCREASE = MIN_AUC_INCREASE, MIN_AUC = MIN_AUC, MIN_COMP_TO_CHECK = MIN_COMP_TO_CHECK,
x.scale = x.scale, x.center = x.center,
y.scale = y.scale, y.center = y.center,
fast_mode = fast_mode, return_models = return_models, MIN_EPV = MIN_EPV,
pred.attr = pred.attr, pred.method = pred.method, seed = seed, PARALLEL = PARALLEL,
verbose = verbose)
cv.splsdacox_mixOmics_res$plot_AUC
cv.splsdacox_mixOmics_res$opt.comp
cv.splsdacox_mixOmics_res$opt.nvar
best_splsdacox_mixOmics <- HDcox::splsdacox_mixOmics(X_train, Y_train,
n.comp = cv.splsdacox_mixOmics_res$opt.comp,
vector = cv.splsdacox_mixOmics_res$opt.nvar,
x.center = x.center, x.scale = x.scale,
y.center = y.center, y.scale = y.scale,
remove_near_zero_variance = remove_near_zero_variance,
remove_zero_variance = remove_zero_variance,
toKeep.zv = toKeep.zv,
remove_non_significant = remove_non_significant,
alpha = alpha,
MIN_EPV = MIN_EPV,
returnData = returnData,
verbose = verbose)
###############
# SAVE MODELS #
###############
lst_models_full <- list("best_cox" = best_cox,
"best_coxSW" = best_coxSW,
"cv.coxEN_res" = cv.coxEN_res,
"coxEN" = best_coxEN,
"cv.plsRcox_res" = cv.plsRcox_res,
"plsRcox" = best_plsRcox,
"cv.splsdrcox_res" = cv.splsdrcox_res,
"splsdrcox" = best_splsdrcox,
"cv.splsdrcox_mixOmics_res" = cv.splsdrcox_mixOmics_res,
"splsdrcox_mixOmics" = best_splsdrcox_mixOmics,
"cv.splsdacox_mixOmics_res" = cv.splsdacox_mixOmics_res,
"splsdacox_mixOmics" = best_splsdacox_mixOmics)
###############
# SAVE MODELS #
###############
lst_models_full <- list("best_cox" = best_cox,
"best_coxSW" = best_coxSW,
"cv.coxEN_res" = cv.coxEN_res,
"coxEN" = best_coxEN,
"cv.plsicox_res" = cv.plsicox_res,
"plsicox" = best_plsicox,
"cv.splsdrcox_res" = cv.splsdrcox_res,
"splsdrcox" = best_splsdrcox,
"cv.splsdrcox_mixOmics_res" = cv.splsdrcox_mixOmics_res,
"splsdrcox_mixOmics" = best_splsdrcox_mixOmics,
"cv.splsdacox_mixOmics_res" = cv.splsdacox_mixOmics_res,
"splsdacox_mixOmics" = best_splsdacox_mixOmics)
lst_models <- list("cox" = best_cox,
"coxSW" = best_coxSW,
"coxEN" = best_coxEN,
"plsicox" = best_plsicox,
"splsdrcox" = best_splsdrcox,
"splsdrcox_mixOmics" = best_splsdrcox_mixOmics,
"splsdacox_mixOmics" = best_splsdacox_mixOmics)
##############
# EVALUATION #
##############
lst_evaluations <- c("survivalROC", "cenROC", "nsROC", "smoothROCtime_C", "smoothROCtime_I", "risksetROC")
names(lst_evaluations) <- lst_evaluations
lst_evaluations <- "cenROC"
eval_results <- purrr::map(lst_evaluations, ~eval_models4.0(lst_models = lst_models,
X_test = X_test, Y_test = Y_test, pred.method = .,
pred.attr = pred.attr, verbose = T,
times = times, max_time_points = 15, PARALLEL = PARALLEL))
#### ### ### #
# EVALPLOTS #
#### ### ### #
evaluation_folder = paste0(folder, "evaluation_plot/")
dir.create(evaluation_folder)
lst_ggp <- plot_evaluation(eval_results)
eval_results
eval_results <- eval_models4.0(lst_models = lst_models,
X_test = X_test, Y_test = Y_test, pred.method = "cenROC",
pred.attr = pred.attr, verbose = T,
times = times, max_time_points = 15, PARALLEL = PARALLEL)
lst_ggp <- plot_evaluation(eval_results)
lst_ggp$lst_plots$lineplot.mean
eval_results <- purrr::map(lst_evaluations, ~eval_models4.0(lst_models = lst_models,
X_test = X_test, Y_test = Y_test, pred.method = .,
pred.attr = pred.attr, verbose = T,
times = times, max_time_points = 15, PARALLEL = PARALLEL))
names(eval_results) <- lst_evaluations
gc()
lst_ggp <- plot_evaluation.list(eval_results)
lst_ggp$cenROC
lst_ggp$cenROC$lst_plots$lineplot$data
pred.attr = "mean"
y.min = NULL
type = "both"
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
df <- eval_results$cenROC$df
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
y.min
y.min <- floor(min(eval_results$cenROC$df$AUC, na.rm = T)*10)/10
y.min
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
eval_results <- eval_results$cenROC
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
if(type == "both"){
lst_ggp <- lst_plots
}else if(type == "line"){
lst_ggp <- lst_plots$lineplot
}else if(type == "mean"){
lst_ggp <- lst_plots$lineplot.mean
}
lst_tests <-c("t.test", "anova","wilcox.test", "kruskal.test", "NULL")
lst_plot_comparisons <- list()
for(t in 1:length(lst_tests)){
if(lst_tests[[t]]!="NULL"){
test_comparations = lst_tests[[t]]
}else{
test_comparations = NULL
}
plot <- boxplot.performance(df = eval_results$df,
x.var = "method",
y.var = "AUC",
x.fill = "method",
x.alpha = NULL,
alpha.lab = NULL,
x.lab = "Method",
y.lab = "AUC",
fill.lab = NULL,
title = paste0("Method Performance"),
y.limit = NULL,
y.limit.exception = NULL,
jitter = F,
test = test_comparations,
show.median = T,
round.median = 3)
if(lst_tests[[t]] == "NULL"){
lst_plot_comparisons[["no_test"]] <- plot
}else{
lst_plot_comparisons[[lst_tests[[t]]]] <- plot
}
df
table <- data.frame("method" = NULL, "metric" = NULL, "mean" = NULL, "median" = NULL, "sd" = NULL)
table
m="cox"
c = "AUC"
mean(df[df$method==m,c])
df[df$method==m,c]
mean(df[df$method==m,c])
class(df[df$method==m,c])
class(df[df$method==m,c,drop=T])
mean(class(df[df$method==m,c,drop=T]))
mean(df[df$method==m,c,drop=T]))
mean(df[df$method==m,c,drop=T])
vector <- c(m, c,
mean(df[df$method==m,c,drop=T]),
median(df[df$method==m,c,drop=T]),
sd(df[df$method==m,c,drop=T]))
vector
table <- data.frame("method" = NULL, "metric" = NULL, "mean" = NULL, "median" = NULL, "sd" = NULL)
for(m in unique(df$method)){
for(c in colnames(df)){
if(c=="method"){
next
}else{
vector <- c(m, c,
mean(df[df$method==m,c,drop=T]),
median(df[df$method==m,c,drop=T]),
sd(df[df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
m
c
table <- data.frame("method" = NULL, "metric" = NULL, "mean" = NULL, "median" = NULL, "sd" = NULL)
for(m in unique(df$method)){
for(c in colnames(df)){
if(c=="method" | c=="time"){
next
}else{
vector <- c(m, c,
mean(df[df$method==m,c,drop=T]),
median(df[df$method==m,c,drop=T]),
sd(df[df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
table
colnames(table) <- c("method","metric","mean","median","sd")
table
table$method
table$metric
table$mean
table$median
table$sd
table$method
table$method <- factor(table$method)
table$metric <- factor(table$metric)
table$mean <- as.numeric(table$mean)
table$median <- as.numeric(table$median)
table$sd <- as.numeric(table$sd)
table
table$median
table$mean
table$sd
load_all()
setwd("D:/Pedro/Mega/Doctorado/Otros proyectos/R/HDcox")
load_all()
save_ggplot_lst.svg(lst_plots = lst_ggp, object_name = "lineplot.mean", folder = evaluation_folder, wide = T, prefix = "eval_")
setwd("D:/Pedro/Mega/Doctorado/Otros proyectos/plsicox/results")
getwd()
devtools::load_all(".")
setwd("D:/Pedro/Mega/Doctorado/Otros proyectos/plsicox/results")
lst_ggp <- plot_evaluation.list(eval_results)
eval_results <- purrr::map(lst_evaluations, ~eval_models4.0(lst_models = lst_models,
X_test = X_test, Y_test = Y_test, pred.method = .,
pred.attr = pred.attr, verbose = T,
times = times, max_time_points = 15, PARALLEL = PARALLEL))
names(eval_results) <- lst_evaluations
lst_ggp <- plot_evaluation.list(eval_results)
df
devtools::load_all(".")
lst_ggp <- plot_evaluation.list(eval_results)
eval_results <- eval_results$cenROC
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
if(type == "both"){
lst_ggp <- lst_plots
}else if(type == "line"){
lst_ggp <- lst_plots$lineplot
}else if(type == "mean"){
lst_ggp <- lst_plots$lineplot.mean
}
lst_tests <-c("t.test", "anova","wilcox.test", "kruskal.test", "NULL")
lst_plot_comparisons <- list()
for(t in 1:length(lst_tests)){
if(lst_tests[[t]]!="NULL"){
test_comparations = lst_tests[[t]]
}else{
test_comparations = NULL
}
plot <- boxplot.performance(df = eval_results$df,
x.var = "method",
y.var = "AUC",
x.fill = "method",
x.alpha = NULL,
alpha.lab = NULL,
x.lab = "Method",
y.lab = "AUC",
fill.lab = NULL,
title = paste0("Method Performance"),
y.limit = NULL,
y.limit.exception = NULL,
jitter = F,
test = test_comparations,
show.median = T,
round.median = 3)
if(lst_tests[[t]] == "NULL"){
lst_plot_comparisons[["no_test"]] <- plot
}else{
lst_plot_comparisons[[lst_tests[[t]]]] <- plot
}
table <- NULL
for(m in unique(eval_results$df)){
for(c in colnames(df)){
if(c=="method" | c=="time"){
next
}else{
vector <- c(m, c,
mean(eval_results$df[eval_results$df$method==m,c,drop=T]),
median(eval_results$df[eval_results$df$method==m,c,drop=T]),
sd(eval_results$df[eval_results$df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
m
unique(eval_results$df$method)
devtools::load_all(".")
eval_results <- purrr::map(lst_evaluations, ~eval_models4.0(lst_models = lst_models,
X_test = X_test, Y_test = Y_test, pred.method = .,
pred.attr = pred.attr, verbose = T,
times = times, max_time_points = 15, PARALLEL = PARALLEL))
names(eval_results) <- lst_evaluations
lst_ggp <- plot_evaluation.list(eval_results)
eval_results <- eval_results$cenROC
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
if(type == "both"){
lst_ggp <- lst_plots
}else if(type == "line"){
lst_ggp <- lst_plots$lineplot
}else if(type == "mean"){
lst_ggp <- lst_plots$lineplot.mean
}
lst_tests <-c("t.test", "anova","wilcox.test", "kruskal.test", "NULL")
lst_plot_comparisons <- list()
for(t in 1:length(lst_tests)){
if(lst_tests[[t]]!="NULL"){
test_comparations = lst_tests[[t]]
}else{
test_comparations = NULL
}
plot <- boxplot.performance(df = eval_results$df,
x.var = "method",
y.var = "AUC",
x.fill = "method",
x.alpha = NULL,
alpha.lab = NULL,
x.lab = "Method",
y.lab = "AUC",
fill.lab = NULL,
title = paste0("Method Performance"),
y.limit = NULL,
y.limit.exception = NULL,
jitter = F,
test = test_comparations,
show.median = T,
round.median = 3)
if(lst_tests[[t]] == "NULL"){
lst_plot_comparisons[["no_test"]] <- plot
}else{
lst_plot_comparisons[[lst_tests[[t]]]] <- plot
}
table <- NULL
for(m in unique(eval_results$df$method)){
for(c in colnames(df)){
if(c=="method" | c=="time"){
next
}else{
vector <- c(m, c,
mean(eval_results$df[eval_results$df$method==m,c,drop=T]),
median(eval_results$df[eval_results$df$method==m,c,drop=T]),
sd(eval_results$df[eval_results$df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
colnames(table) <- c("method","metric","mean","median","sd")
table$method <- factor(table$method)
table$metric <- factor(table$metric)
table$mean <- as.numeric(table$mean)
table
class(table)
table <- as.data.frame(table)
rownames(table) <- NULL
colnames(table) <- c("method","metric","mean","median","sd")
table$method <- factor(table$method)
table$metric <- factor(table$metric)
table$mean <- as.numeric(table$mean)
table$median <- as.numeric(table$median)
table$sd <- as.numeric(table$sd)
table
devtools::load_all(".")
eval_results <- purrr::map(lst_evaluations, ~eval_models4.0(lst_models = lst_models,
X_test = X_test, Y_test = Y_test, pred.method = .,
pred.attr = pred.attr, verbose = T,
times = times, max_time_points = 15, PARALLEL = PARALLEL))
names(eval_results) <- lst_evaluations
lst_ggp <- plot_evaluation.list(eval_results)
lst_ggp <- plot_evaluation(eval_results$cenROC)
aaa <- eval_results
eval_results = eval_results$cenROC
pred.attr = "mean"
y.min = NULL
type = "both"
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
if(type == "both"){
lst_ggp <- lst_plots
}else if(type == "line"){
lst_ggp <- lst_plots$lineplot
}else if(type == "mean"){
lst_ggp <- lst_plots$lineplot.mean
}
lst_tests <-c("t.test", "anova","wilcox.test", "kruskal.test", "NULL")
lst_plot_comparisons <- list()
for(t in 1:length(lst_tests)){
if(lst_tests[[t]]!="NULL"){
test_comparations = lst_tests[[t]]
}else{
test_comparations = NULL
}
plot <- boxplot.performance(df = eval_results$df,
x.var = "method",
y.var = "AUC",
x.fill = "method",
x.alpha = NULL,
alpha.lab = NULL,
x.lab = "Method",
y.lab = "AUC",
fill.lab = NULL,
title = paste0("Method Performance"),
y.limit = NULL,
y.limit.exception = NULL,
jitter = F,
test = test_comparations,
show.median = T,
round.median = 3)
if(lst_tests[[t]] == "NULL"){
lst_plot_comparisons[["no_test"]] <- plot
}else{
lst_plot_comparisons[[lst_tests[[t]]]] <- plot
}
table <- NULL
for(m in unique(eval_results$df$method)){
for(c in colnames(df)){
if(c=="method" | c=="time"){
next
}else{
vector <- c(m, c,
mean(eval_results$df[eval_results$df$method==m,c,drop=T]),
median(eval_results$df[eval_results$df$method==m,c,drop=T]),
sd(eval_results$df[eval_results$df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
table <- as.data.frame(table)
rownames(table) <- NULL
colnames(table) <- c("method","metric","mean","median","sd")
table$method <- factor(table$method)
table$metric <- factor(table$metric)
table$mean <- as.numeric(table$mean)
table$median <- as.numeric(table$median)
table$sd <- as.numeric(table$sd)
df
table
load_all()
lst_ggp <- HDcox::plot_evaluation(eval_results = eval_results$cenROC)
lst_ggp <- plot_evaluation(eval_results = eval_results$cenROC)
lst_ggp <- plot_evaluation.list(eval_results)
eval_results <- aaa
lst_ggp <- plot_evaluation.list(eval_results)
save.image("C:/Users/pedro/Desktop/HDcox_cancer.RData")
devtools::load_all(".")
remove.packages("HDcox")
library(devtools)
devtools::load_all(".")
check()
