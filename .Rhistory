names(eval_results) <- lst_evaluations
gc()
lst_ggp <- plot_evaluation.list(eval_results)
lst_ggp$cenROC
lst_ggp$cenROC$lst_plots$lineplot$data
pred.attr = "mean"
y.min = NULL
type = "both"
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
df <- eval_results$cenROC$df
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
y.min
y.min <- floor(min(eval_results$cenROC$df$AUC, na.rm = T)*10)/10
y.min
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
eval_results <- eval_results$cenROC
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
if(type == "both"){
lst_ggp <- lst_plots
}else if(type == "line"){
lst_ggp <- lst_plots$lineplot
}else if(type == "mean"){
lst_ggp <- lst_plots$lineplot.mean
}
lst_tests <-c("t.test", "anova","wilcox.test", "kruskal.test", "NULL")
lst_plot_comparisons <- list()
for(t in 1:length(lst_tests)){
if(lst_tests[[t]]!="NULL"){
test_comparations = lst_tests[[t]]
}else{
test_comparations = NULL
}
plot <- boxplot.performance(df = eval_results$df,
x.var = "method",
y.var = "AUC",
x.fill = "method",
x.alpha = NULL,
alpha.lab = NULL,
x.lab = "Method",
y.lab = "AUC",
fill.lab = NULL,
title = paste0("Method Performance"),
y.limit = NULL,
y.limit.exception = NULL,
jitter = F,
test = test_comparations,
show.median = T,
round.median = 3)
if(lst_tests[[t]] == "NULL"){
lst_plot_comparisons[["no_test"]] <- plot
}else{
lst_plot_comparisons[[lst_tests[[t]]]] <- plot
}
df
table <- data.frame("method" = NULL, "metric" = NULL, "mean" = NULL, "median" = NULL, "sd" = NULL)
table
m="cox"
c = "AUC"
mean(df[df$method==m,c])
df[df$method==m,c]
mean(df[df$method==m,c])
class(df[df$method==m,c])
class(df[df$method==m,c,drop=T])
mean(class(df[df$method==m,c,drop=T]))
mean(df[df$method==m,c,drop=T]))
mean(df[df$method==m,c,drop=T])
vector <- c(m, c,
mean(df[df$method==m,c,drop=T]),
median(df[df$method==m,c,drop=T]),
sd(df[df$method==m,c,drop=T]))
vector
table <- data.frame("method" = NULL, "metric" = NULL, "mean" = NULL, "median" = NULL, "sd" = NULL)
for(m in unique(df$method)){
for(c in colnames(df)){
if(c=="method"){
next
}else{
vector <- c(m, c,
mean(df[df$method==m,c,drop=T]),
median(df[df$method==m,c,drop=T]),
sd(df[df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
m
c
table <- data.frame("method" = NULL, "metric" = NULL, "mean" = NULL, "median" = NULL, "sd" = NULL)
for(m in unique(df$method)){
for(c in colnames(df)){
if(c=="method" | c=="time"){
next
}else{
vector <- c(m, c,
mean(df[df$method==m,c,drop=T]),
median(df[df$method==m,c,drop=T]),
sd(df[df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
table
colnames(table) <- c("method","metric","mean","median","sd")
table
table$method
table$metric
table$mean
table$median
table$sd
table$method
table$method <- factor(table$method)
table$metric <- factor(table$metric)
table$mean <- as.numeric(table$mean)
table$median <- as.numeric(table$median)
table$sd <- as.numeric(table$sd)
table
table$median
table$mean
table$sd
load_all()
setwd("D:/Pedro/Mega/Doctorado/Otros proyectos/R/HDcox")
load_all()
save_ggplot_lst.svg(lst_plots = lst_ggp, object_name = "lineplot.mean", folder = evaluation_folder, wide = T, prefix = "eval_")
setwd("D:/Pedro/Mega/Doctorado/Otros proyectos/plsicox/results")
getwd()
devtools::load_all(".")
setwd("D:/Pedro/Mega/Doctorado/Otros proyectos/plsicox/results")
lst_ggp <- plot_evaluation.list(eval_results)
eval_results <- purrr::map(lst_evaluations, ~eval_models4.0(lst_models = lst_models,
X_test = X_test, Y_test = Y_test, pred.method = .,
pred.attr = pred.attr, verbose = T,
times = times, max_time_points = 15, PARALLEL = PARALLEL))
names(eval_results) <- lst_evaluations
lst_ggp <- plot_evaluation.list(eval_results)
df
devtools::load_all(".")
lst_ggp <- plot_evaluation.list(eval_results)
eval_results <- eval_results$cenROC
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
if(type == "both"){
lst_ggp <- lst_plots
}else if(type == "line"){
lst_ggp <- lst_plots$lineplot
}else if(type == "mean"){
lst_ggp <- lst_plots$lineplot.mean
}
lst_tests <-c("t.test", "anova","wilcox.test", "kruskal.test", "NULL")
lst_plot_comparisons <- list()
for(t in 1:length(lst_tests)){
if(lst_tests[[t]]!="NULL"){
test_comparations = lst_tests[[t]]
}else{
test_comparations = NULL
}
plot <- boxplot.performance(df = eval_results$df,
x.var = "method",
y.var = "AUC",
x.fill = "method",
x.alpha = NULL,
alpha.lab = NULL,
x.lab = "Method",
y.lab = "AUC",
fill.lab = NULL,
title = paste0("Method Performance"),
y.limit = NULL,
y.limit.exception = NULL,
jitter = F,
test = test_comparations,
show.median = T,
round.median = 3)
if(lst_tests[[t]] == "NULL"){
lst_plot_comparisons[["no_test"]] <- plot
}else{
lst_plot_comparisons[[lst_tests[[t]]]] <- plot
}
table <- NULL
for(m in unique(eval_results$df)){
for(c in colnames(df)){
if(c=="method" | c=="time"){
next
}else{
vector <- c(m, c,
mean(eval_results$df[eval_results$df$method==m,c,drop=T]),
median(eval_results$df[eval_results$df$method==m,c,drop=T]),
sd(eval_results$df[eval_results$df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
m
unique(eval_results$df$method)
devtools::load_all(".")
eval_results <- purrr::map(lst_evaluations, ~eval_models4.0(lst_models = lst_models,
X_test = X_test, Y_test = Y_test, pred.method = .,
pred.attr = pred.attr, verbose = T,
times = times, max_time_points = 15, PARALLEL = PARALLEL))
names(eval_results) <- lst_evaluations
lst_ggp <- plot_evaluation.list(eval_results)
eval_results <- eval_results$cenROC
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
if(type == "both"){
lst_ggp <- lst_plots
}else if(type == "line"){
lst_ggp <- lst_plots$lineplot
}else if(type == "mean"){
lst_ggp <- lst_plots$lineplot.mean
}
lst_tests <-c("t.test", "anova","wilcox.test", "kruskal.test", "NULL")
lst_plot_comparisons <- list()
for(t in 1:length(lst_tests)){
if(lst_tests[[t]]!="NULL"){
test_comparations = lst_tests[[t]]
}else{
test_comparations = NULL
}
plot <- boxplot.performance(df = eval_results$df,
x.var = "method",
y.var = "AUC",
x.fill = "method",
x.alpha = NULL,
alpha.lab = NULL,
x.lab = "Method",
y.lab = "AUC",
fill.lab = NULL,
title = paste0("Method Performance"),
y.limit = NULL,
y.limit.exception = NULL,
jitter = F,
test = test_comparations,
show.median = T,
round.median = 3)
if(lst_tests[[t]] == "NULL"){
lst_plot_comparisons[["no_test"]] <- plot
}else{
lst_plot_comparisons[[lst_tests[[t]]]] <- plot
}
table <- NULL
for(m in unique(eval_results$df$method)){
for(c in colnames(df)){
if(c=="method" | c=="time"){
next
}else{
vector <- c(m, c,
mean(eval_results$df[eval_results$df$method==m,c,drop=T]),
median(eval_results$df[eval_results$df$method==m,c,drop=T]),
sd(eval_results$df[eval_results$df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
colnames(table) <- c("method","metric","mean","median","sd")
table$method <- factor(table$method)
table$metric <- factor(table$metric)
table$mean <- as.numeric(table$mean)
table
class(table)
table <- as.data.frame(table)
rownames(table) <- NULL
colnames(table) <- c("method","metric","mean","median","sd")
table$method <- factor(table$method)
table$metric <- factor(table$metric)
table$mean <- as.numeric(table$mean)
table$median <- as.numeric(table$median)
table$sd <- as.numeric(table$sd)
table
devtools::load_all(".")
eval_results <- purrr::map(lst_evaluations, ~eval_models4.0(lst_models = lst_models,
X_test = X_test, Y_test = Y_test, pred.method = .,
pred.attr = pred.attr, verbose = T,
times = times, max_time_points = 15, PARALLEL = PARALLEL))
names(eval_results) <- lst_evaluations
lst_ggp <- plot_evaluation.list(eval_results)
lst_ggp <- plot_evaluation(eval_results$cenROC)
aaa <- eval_results
eval_results = eval_results$cenROC
pred.attr = "mean"
y.min = NULL
type = "both"
if(!pred.attr %in% c("mean", "median")){
stop("pred.attr parameter must be one of: 'mean' or 'median'")
}
if(!type %in% c("both", "line", "mean")){
type = "both"
}
#select minimum for all evals
if(is.null(y.min)){
y.min <- floor(min(eval_results$df$AUC, na.rm = T)*10)/10
}
lst_ggp <- list()
lst_plots <- comboplot.performance2.0(df = eval_results$df,
x.var = "time", y.var = "AUC", x.color = "method",
y.limit = c(y.min, 1), pred.attr = pred.attr)
if(type == "both"){
lst_ggp <- lst_plots
}else if(type == "line"){
lst_ggp <- lst_plots$lineplot
}else if(type == "mean"){
lst_ggp <- lst_plots$lineplot.mean
}
lst_tests <-c("t.test", "anova","wilcox.test", "kruskal.test", "NULL")
lst_plot_comparisons <- list()
for(t in 1:length(lst_tests)){
if(lst_tests[[t]]!="NULL"){
test_comparations = lst_tests[[t]]
}else{
test_comparations = NULL
}
plot <- boxplot.performance(df = eval_results$df,
x.var = "method",
y.var = "AUC",
x.fill = "method",
x.alpha = NULL,
alpha.lab = NULL,
x.lab = "Method",
y.lab = "AUC",
fill.lab = NULL,
title = paste0("Method Performance"),
y.limit = NULL,
y.limit.exception = NULL,
jitter = F,
test = test_comparations,
show.median = T,
round.median = 3)
if(lst_tests[[t]] == "NULL"){
lst_plot_comparisons[["no_test"]] <- plot
}else{
lst_plot_comparisons[[lst_tests[[t]]]] <- plot
}
table <- NULL
for(m in unique(eval_results$df$method)){
for(c in colnames(df)){
if(c=="method" | c=="time"){
next
}else{
vector <- c(m, c,
mean(eval_results$df[eval_results$df$method==m,c,drop=T]),
median(eval_results$df[eval_results$df$method==m,c,drop=T]),
sd(eval_results$df[eval_results$df$method==m,c,drop=T]))
table <- rbind(table, vector)
}
table <- as.data.frame(table)
rownames(table) <- NULL
colnames(table) <- c("method","metric","mean","median","sd")
table$method <- factor(table$method)
table$metric <- factor(table$metric)
table$mean <- as.numeric(table$mean)
table$median <- as.numeric(table$median)
table$sd <- as.numeric(table$sd)
df
table
load_all()
lst_ggp <- HDcox::plot_evaluation(eval_results = eval_results$cenROC)
lst_ggp <- plot_evaluation(eval_results = eval_results$cenROC)
lst_ggp <- plot_evaluation.list(eval_results)
eval_results <- aaa
lst_ggp <- plot_evaluation.list(eval_results)
save.image("C:/Users/pedro/Desktop/HDcox_cancer.RData")
devtools::load_all(".")
remove.packages("HDcox")
library(devtools)
devtools::load_all(".")
check()
remove.packages("HDcox")
#libraries
library(HDcox)
library(RColorConesa) #from GitHub #devtools #usethis and gert install.packages('hrbrthemes', repos='http://cran.us.r-project.org')
#Load ggplot theme
loadGgplotTheme <- function(path){
file <- paste0(path,"ggplot_theme.R")
source(file, echo = F)
}
path <- "D:/Pedro/Mega/Doctorado/Otros proyectos/"
loadGgplotTheme(path)
load("D:/Pedro/Mega/Doctorado/Otros proyectos/data_cancer.RData")
NAME = "CANCER_"
# METHODS
lst_evaluations <- c("survivalROC", "cenROC", "nsROC", "smoothROCtime_C", "smoothROCtime_I", "risksetROC")
names(lst_evaluations) <- lst_evaluations
# Classical
FLAG_COX = T
FLAG_COXSW = T
# hd
FLAG_COXEN = T
FLAG_PLSICOX = T
FLAG_sPLSDRCOX = T
FLAG_sPLSDRCOX_MO = T
FLAG_sPLSDACOX_MO = T
# mo
FLAG_SB.PLSICOX = F
FLAG_FAST.SB.PLSICOX = F
FLAG_SB.sPLSDRCOX = F
FLAG_FAST.SB.sPLSDRCOX = F
FLAG_MB.sPLSDRCOX = F
FLAG_MB.sPLSDACOX = F
# Scale Parameters
x.center = T
x.scale = T
y.center = F
y.scale = F
# Survival Parameters
MIN_EPV = 5
pred.attr = "mean"
# Algorithm Parameters
remove_non_significant = F
remove_non_significant_models = F
remove_near_zero_variance = T
remove_zero_variance = F
toKeep.zv = NULL
alpha = 0.05
returnData = T
verbose = T
PARALLEL = T
# cox
FORCE = T
# SW
boostDeletion = F
BACKWARDS = T
initialModel = "NULL"
toKeep.sw = NULL
alpha_ENT = 0.1
alpha_OUT = 0.15
alpha_PH  = 0.05
check_PH = F
# coxEN
EN.alpha.list = seq(0,1,0.1)
# PLS
max.ncomp = 10
max.iter = 500
# Cross Validation Parameters
times = NULL
return_models = F
seed = 123
# Weights Parameters
w_AIC = 0
w_c.index = 0
w_AUC = 1
# sPLS-DR-cox
eta.list = seq(0,0.9,0.25)
# mixOmics
vector = NULL
MIN_NVAR = 10
MAX_NVAR = 1000
n.cut_points = 5
# Eval stop detection
MIN_AUC_INCREASE = 0.01 # 1%
MIN_AUC = 0.75 # 75%
MIN_COMP_TO_CHECK = 3
# Model Lists
lst_models_full <- NULL
lst_models <- NULL
# Evaluation multiple models
max_time_points = 15
#### ### ### #### ### ### ###
#Cross Validation Parameters #
#### ### ### ### ### #### ###
n_run = 5
k_folds = 10
fast_mode = F
pred.method = "cenROC"
todaydate <- format(Sys.time(), '%Y-%m-%d')
txt_folder <- paste0(NAME,ifelse(fast_mode, "FAST_", "COMPLETE_"), pred.method, "_runs_", n_run, "_folds_", k_folds)
folder <- paste0(txt_folder,"_",todaydate,"/")
setwd("D:/Pedro/Mega/Doctorado/Otros proyectos/plsicox/results")
